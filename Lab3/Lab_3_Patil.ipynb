{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import pdfminer\n",
    "import glob\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "import xgboost\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "#This block is just for importing relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Read Shannon's 1948 paper 'A Mathematical Theory of Communication'. Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication. http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf Summarize what you learned briely (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon's paper, \"A Mathematical Theory of Communication\", aims to tackle a fundamental problem of communication (as of 1948) relating to noiseless communication systems. The first part of the paper provides a brief intuition for the choice of the logarithmic function for information transfer, and then defines a communication system in terms of five components. The first two components are information source which provides the information to be transmit, and transmitter, which encodes the message in the form of a signal that can be transmit. The last two components are receiver and destination, which are the inverse of the transmitter and source, respectively. The transmitter and receiver are connected by a channel which is the medium used to transmit the signal, and that is where noise is likely to be introduced.\n",
    "\n",
    "The next few subsections look at the mathematial properties of discrete noiseless systems. Starting with the capacity of a channel, there is a brief discussion on allowable sequences, sources of information, approximations and n-grams. Generally, these sections try to lay the foundation for modern natural language processing (NLP), by talking about series of approximations to english, establishing sentences as a Markov (Markoff?) process and specifically, ergodic processes.\n",
    "\n",
    "After this formulation, a measure of uncertainity is introduced. This measure is entropy, and it has to follow three properties relating to the probaility distribution, i.e., continuity, monotonically increasing funcion of number of choices and indifference to successive choices. The only function satisfying said properties is the proposed \"Shannon\" formula which defines entropy as:\n",
    "\n",
    "$H = -K \\Sigma_{i=1}^n p_i logp_i$\n",
    "\n",
    "A few properties of this formula are explored such as behavior at extremes, behavior under joint distributions, conditional entropy, etc. The last few subsections talk about application of entropy to an information source, and how encoding/decoding operations can be represented to minimize the number of required bits. The fundamental theorem for a noiseless channel provides a hard upper bound for the avergae symbols per second transmitted for a given channel. Lastly, an example is provided to show how the average number of bits is obtained for a toy example with a special encoding scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "## Scraping, Entropy and ICML papers\n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2017 papers\n",
    "from http://proceedings.mlr.press/v70/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "\n",
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of Z.\n",
    "\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "\n",
    "4. (Extra credit) Synthesize a random paragraph using an n-gram model on words. Synthesize a random paragraph using any model you want. Top five synthesized text paragraphs win bonus (+30 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "This code block below reads in all pdf files and pastes the plaintext into one txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfConverter:\n",
    "\n",
    "   def __init__(self, file_path):\n",
    "       self.file_path = file_path\n",
    "# convert pdf file to a string which has space among words \n",
    "   def convert_pdf_to_txt(self):\n",
    "       rsrcmgr = PDFResourceManager()\n",
    "       retstr = StringIO()\n",
    "       codec = 'utf-8'  # 'utf16','utf-8'\n",
    "       laparams = LAParams()\n",
    "       device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "       fp = open(self.file_path, 'rb')\n",
    "       interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "       password = \"\"\n",
    "       maxpages = 0\n",
    "       caching = True\n",
    "       pagenos = set()\n",
    "       for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "           interpreter.process_page(page)\n",
    "       fp.close()\n",
    "       device.close()\n",
    "       str = retstr.getvalue()\n",
    "       retstr.close()\n",
    "       return str\n",
    "# convert pdf file text to string and save as a text_pdf.txt file\n",
    "   def save_convert_pdf_to_txt(self):\n",
    "       content = self.convert_pdf_to_txt()\n",
    "       txt_pdf = open('text_pdf.txt', 'ab')\n",
    "       txt_pdf.write(content.encode('utf-8'))\n",
    "       txt_pdf.close()\n",
    "        \n",
    "pdflist = glob.glob(r\"C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\*.pdf\")\n",
    "\n",
    "for pdf in pdflist:\n",
    "    print(\"Working on: \" + pdf + '\\n')\n",
    "    pdfConverter = PdfConverter(file_path=pdf)\n",
    "    #print(pdfConverter.convert_pdf_to_txt())\n",
    "    pdfConverter.save_convert_pdf_to_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the stored plain text from pdfs\n",
    "with open (\"text_pdf.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "    text=myfile.readlines()\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# Tokenize and build vocabulary from the text corpus\n",
    "vectorizer.fit(text)\n",
    "# Summarize the vocabulary\n",
    "data = vectorizer.vocabulary_\n",
    "\n",
    "# This writes the words and counts to a txt file\n",
    "with open('Word_counts.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    print(vectorizer.vocabulary_, file=f)\n",
    "\n",
    "# A function is defined to return top n frequent words from a vocabulary list\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "get_top_n_words(text, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts the counts to raw probabilities of appearance, and drops zero value words, i.e., so rare that their probability was rounded down to zero.\n",
    "vec = CountVectorizer().fit(text)\n",
    "bag_of_words = vec.transform(text)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "print(type(words_freq))\n",
    "Prob_dist_dict = dict()\n",
    "def Convert(tup, di): \n",
    "    for a, b in tup: \n",
    "        di.setdefault(a, []).append(b) \n",
    "    return di \n",
    "Convert(words_freq, Prob_dist_dict)\n",
    "\n",
    "#print(Prob_dist_dict)\n",
    "\n",
    "robs = list(Prob_dist_dict.values())\n",
    "flatProbs = [ item for elem in Probs for item in elem]\n",
    "#print(Probs)\n",
    "sum_prob = sum(flatProbs)\n",
    "for x in range(len(Probs)):\n",
    "    flatProbs[x] = flatProbs[x]/sum_prob\n",
    "\n",
    "#print(flatProbs)\n",
    "\n",
    "# Entropy calculation below\n",
    "entropy = 0\n",
    "for i in range(len(flatProbs)):\n",
    "    entropy = entropy + (flatProbs[i]* math.log2(flatProbs[i]))\n",
    "entropy = entropy*(-1)\n",
    "print(\"The Shannon entropy is calculated to be: \",entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block below returns n words chosen according to their probability of appearance\n",
    "# This is a \"paragraph\" of 100 words.\n",
    "n=100\n",
    "keys = np.array(list(Prob_dist_dict.keys()))\n",
    "Probs = list(Prob_dist_dict.values())\n",
    "flatProbs = [ item for elem in Probs for item in elem]\n",
    "#print(Probs)\n",
    "sum_prob = sum(flatProbs)\n",
    "for x in range(len(Probs)):\n",
    "    flatProbs[x] = flatProbs[x]/sum_prob\n",
    "np.random.seed(2)\n",
    "choice_list = np.random.choice(keys, n, replace=True, p=flatProbs)\n",
    "print(choice_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block below tokenizes the word corpus\n",
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n",
    "\n",
    "\n",
    "#This tokenizes our text saved in variable text\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(str(text))]\n",
    "\n",
    "# Preprocess the tokenized text for n-grams language modelling\n",
    "n = 5\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "\n",
    "model = MLE(n)\n",
    "print(\"The n-gram model is training now...\")\n",
    "model.fit(train_data, padded_sents)\n",
    "print(\"The model has been trained successfully. The details are as follows:\")\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sentence(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"The random sentence number \",i,\" is: \")\n",
    "    print(generate_sentence(model, 200, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "## Starting in Kaggle\n",
    "\n",
    "Soon you will be participating in the in-class Kaggle competition made for this class. In that one, you will be participating on your own. This is a warmup- the more e\u000b",
    "ort and research you put into this assignment the easier it will be to compete into the real Kaggle competition that you will need to do soon. We expect you to spend 10 times more e\u000b",
    "ort on this problem compared to the others.\n",
    "\n",
    "1. Let's start with our first Kaggle submission in a playground regression competition. Make an account to Kaggle and find https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "\n",
    "2. Follow the data preprocessing steps from https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models. Then run a ridge regression using $\\alpha = 0.1$. Make a submission of this prediction, what is the RMSE you get? (Hint: remember to exponentiate np.expm1(ypred) your predictions).\n",
    "\n",
    "3. Compare a ridge regression and a lasso regression model. Optimize the alphas using cross validation. What is the best score you can get from a single ridge regression model and from a single lasso model?\n",
    "\n",
    "4. Plot the l0 norm (number of nonzeros) of the coefficients that lasso produces as you vary the strength of regularization parameter alpha.\n",
    "\n",
    "5. Add the outputs of your models as features and train a ridge regression on all the features plus the model outputs (This is called Ensembling and Stacking). Be careful not to overfit. What score can you get? (We will be discussing ensembling more, later in the class, but you can start playing with it now).\n",
    "\n",
    "6. Install XGBoost (Gradient Boosting) and train a gradient boosting regression. What score can you get just from a single XGB? (you will need to optimize over its parameters). We will discuss boosting and gradient boosting in more detail later. XGB is a great friend to all good Kagglers!\n",
    "\n",
    "7. Do your best to get the more accurate model. Try feature engineering and stacking many models. You are allowed to use any public tool in python. No non-python tools allowed.\n",
    "\n",
    "8. (Optional) Read the Kaggle forums, tutorials and Kernels in this competition. This is an excellent way to learn. Include in your report if you find something in the forums you like, or if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
    "\n",
    "9. Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle forums. Every student should have their own individual Kaggle account and teams can be formed in the Kaggle submissions with partners. This is more important for live competitions of course.\n",
    "\n",
    "10. As in the real in-class Kaggle competition (which will be next), you will be graded based on your public score (include that in your report) and also on the creativity of your solution. In your report (that you will submit as a pdf file), explain what worked and what did not work. Many creative things will not work, but you will get partial credit for developing them. We will invite teams with interesting solutions to present them in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block reads the necessary input files\n",
    "def load_data(data_path='data/'):\n",
    "    train = os.path.join(data_path, \"train.csv\")\n",
    "    test = os.path.join(data_path, \"test.csv\")\n",
    "    return pd.read_csv(train), pd.read_csv(test)\n",
    "\n",
    "train_df, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.DataFrame({\"price\":train_df[\"SalePrice\"], \"log(price + 1)\":np.log1p(train_df[\"SalePrice\"])})\n",
    "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
    "sns.distplot(train_df['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Id column, and the target variable\n",
    "\n",
    "train = train_df.drop('Id', axis=1)\n",
    "\n",
    "test_ids = test_df['Id'].copy()\n",
    "test = test_df.drop('Id', axis=1)\n",
    "\n",
    "train.loc[:,'Train'] = 1\n",
    "test.loc[:,'Train'] = 0\n",
    "\n",
    "housing_df = pd.concat([train,test], ignore_index=True)\n",
    "\n",
    "train_labels = train[\"SalePrice\"].copy()\n",
    "train = train.drop(\"SalePrice\", axis=1) # drop labels for training set\n",
    "\n",
    "#### Type of features\n",
    "\n",
    "train['MSSubClass'] = train['MSSubClass'].astype(str)\n",
    "test['MSSubClass'] = test['MSSubClass'].astype(str)\n",
    "\n",
    "num_attribs = train.select_dtypes([np.number]).columns\n",
    "\n",
    "cat_attribs = train.select_dtypes(include=[np.object]).columns\n",
    "\n",
    "print('numerical:{} \\n\\n categorical:{}'.format(num_attribs, cat_attribs))\n",
    "\n",
    "# Log transform the columns with high skew\n",
    "\n",
    "skewed_cols = num_attribs[train[num_attribs].skew() > 0.75]\n",
    "train[skewed_cols] = np.log1p(train[skewed_cols])\n",
    "test[skewed_cols] = np.log1p(test[skewed_cols])\n",
    "\n",
    "train.head(5)\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown='ignore',sparse=False)),\n",
    "    ])\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(train)\n",
    "\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.1)\n",
    "ridge_reg.fit(train_prepared, train_labels)\n",
    "\n",
    "train_predictions = ridge_reg.predict(train_prepared)\n",
    "ridge_mse = mean_squared_error(train_labels, train_predictions)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "print('rmse on training:', ridge_rmse)\n",
    "\n",
    "test_prepared = full_pipeline.transform(test)\n",
    "test_predictions = ridge_reg.predict(test_prepared)\n",
    "test_predictions = np.expm1(test_predictions)\n",
    "\n",
    "def prep_to_submit(ids, preds, fname='submission.csv'):\n",
    "    preds = pd.DataFrame({'Id': ids, 'SalePrice': preds})\n",
    "    preds.to_csv(fname, index=False)\n",
    "    \n",
    "prep_to_submit(test_ids, test_predictions, fname='submission_ridge.csv')\n",
    "print('this submission scored: ', 0.13636)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge vs Lasso / Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(train_prepared, train_labels)\n",
    "\n",
    "train_predictions = lasso_reg.predict(train_prepared)\n",
    "lasso_mse = mean_squared_error(train_labels, train_predictions)\n",
    "lasso_rmse = np.sqrt(lasso_mse)\n",
    "print('At alpha=0.1: ridge_rmse: {}, lasso_rmse: {}'.format(ridge_rmse, lasso_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the current parameters, ridge seems to perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]}]\n",
    "\n",
    "ridge_reg = Ridge()\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge_reg, param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "lasso_grid = GridSearchCV(lasso_reg, param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "ridge_grid.fit(train_prepared, train_labels)\n",
    "lasso_grid.fit(train_prepared, train_labels)\n",
    "\n",
    "for mean_score, params in zip(ridge_grid.cv_results_[\"mean_test_score\"], ridge_grid.cv_results_[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    \n",
    "for mean_score, params in zip(lasso_grid.cv_results_[\"mean_test_score\"], lasso_grid.cv_results_[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    \n",
    "print('At tuned alphas - best scores: ridge_rmse: {}, lasso_rmse: {}'.format(min(np.sqrt(-ridge_grid.cv_results_['mean_test_score'])), min(np.sqrt(-lasso_grid.cv_results_['mean_test_score']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the optimized alphas, lasso performed slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(model, X, y, cv=10):\n",
    "    scores = cross_val_score(model, X, y, n_jobs=-1, scoring='neg_mean_squared_error', cv=cv)\n",
    "    print(str(model.__class__.__name__) + '; mean_rmse: {}'.format((np.sqrt(-scores)).mean()) + ', std_rmse: {}'.format((np.sqrt(-scores)).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nonzero coefficients Lasso vs regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1e-4, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "num_nonzeros = []\n",
    "for alpha in alphas:\n",
    "    lasso_reg = Lasso(alpha=alpha)\n",
    "    lasso_reg.fit(train_prepared, train_labels)\n",
    "    num_nonzeros.append(sum(lasso_reg.coef_>1e-3))\n",
    "    \n",
    "plt.plot(alphas, num_nonzeros)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('nonzero coefficient count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling and stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('r', ridge_grid),\n",
    "              ('l', lasso_grid)\n",
    "             ]\n",
    "\n",
    "stacking_model = StackingRegressor(estimators=estimators, passthrough=True)\n",
    "#passthrough param if set true trains the final estimator both on training data and prev predictors predictions\n",
    "stacking_model.fit(train_prepared, train_labels);\n",
    "display_scores(stacking_model, train_prepared, train_labels)\n",
    "\n",
    "test_prepared = full_pipeline.transform(test)\n",
    "test_predictions = stacking_model.predict(test_prepared)\n",
    "test_predictions = np.expm1(test_predictions)\n",
    "\n",
    "prep_to_submit(test_ids, test_predictions, fname='submission_stacking_model.csv')\n",
    "print('Score from the described stacking method in the problem: ', 0.13344)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked model performed slightly better than ridge regression submission from 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "        'max_depth': randint(low=1, high=20),\n",
    "        'eta': [0.01, 0.05, 0.1, 0.5, 1],\n",
    "        'subsample' : [0.1, 0.25, 0.5, 0.75, 1],\n",
    "        'n_estimators' : randint(low=150, high=500),\n",
    "    }\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(silent=True, early_stopping_rounds=5)\n",
    "\n",
    "rnd_search = RandomizedSearchCV(xgb_reg, param_distributions=param_distribs,\n",
    "                                n_iter=20, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(train_prepared, train_labels)\n",
    "\n",
    "\n",
    "display_scores(rnd_search, train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost performed the best amongst the other model tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to make a more accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from part 1\n",
    "\n",
    "def load_data(data_path='data/'):\n",
    "    train = os.path.join(data_path, \"train.csv\")\n",
    "    test = os.path.join(data_path, \"test.csv\")\n",
    "    return pd.read_csv(train), pd.read_csv(test)\n",
    "\n",
    "import joblib\n",
    "def save_model(model, fname=\"model.pkl\"):\n",
    "    joblib.dump(model, fname)\n",
    "def load_model(fname):\n",
    "    return joblib.load(fname)\n",
    "\n",
    "def display_scores(model, X, y, cv=10):\n",
    "    scores = cross_val_score(model, X, y, n_jobs=-1, scoring='neg_mean_squared_error', cv=cv)\n",
    "    print(str(model.__class__.__name__) + '; mean_rmse: {:.4f} w std ({:.4f})'.format((np.sqrt(-scores)).mean(), (np.sqrt(-scores)).std()))\n",
    "    return scores \n",
    "\n",
    "def prep_to_submit(ids, test, model):\n",
    "    prepared = full_pipeline.transform(test)\n",
    "    preds = model.predict(prepared)\n",
    "    preds = np.expm1(preds)\n",
    "    preds_df = pd.DataFrame({'Id': ids, 'SalePrice': preds})\n",
    "    preds_df.to_csv(model.__class__.__name__ + '_preds.csv', index=False)\n",
    "    \n",
    "\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "test_ids = test_df['Id'].copy()\n",
    "\n",
    "train_df.drop('Id', axis=1, inplace=True)\n",
    "test_df.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "sns.distplot(train_df['SalePrice']);\n",
    "\n",
    "prices = pd.DataFrame({\"price\":train_df[\"SalePrice\"], \"log(price + 1)\":np.log1p(train_df[\"SalePrice\"])})\n",
    "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
    "sns.distplot(train_df['SalePrice']);\n",
    "\n",
    "corrmat = train_df.corr()\n",
    "plt.subplots(figsize=(15,12));\n",
    "sns.heatmap(corrmat, vmax=0.9, cmap=\"Blues\", square=True);\n",
    "\n",
    "#looks GarageCars and Garage Area are highly correlated with each other, which is expected.\n",
    "#looks like TotRmsAbvGrd and GrLivArea are highly correlated\n",
    "\n",
    "\n",
    "\n",
    "corrmat['SalePrice'].sort_values(ascending=False)[:10]\n",
    "\n",
    "\n",
    "#Let's investigate highly correlated features\n",
    "\n",
    "\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(train_df[cols], size = 2.5)\n",
    "plt.show();\n",
    "\n",
    "#here are 2 data points in GrLiveArea vs Sale Price that looks like an outlier\n",
    "#There is also another data point in TotalBsmtSF vs Sale price taht looks like an outlier\n",
    "#Lets deal with these\n",
    "\n",
    "plt.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'])\n",
    "plt.ylabel('SalePrice')\n",
    "plt.xlabel('GrLivArea')\n",
    "plt.show()\n",
    "\n",
    "#lets drop those 2 examples that looks a lot like outliers\n",
    "\n",
    "\n",
    "\n",
    "train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<12.5)].index)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(x = train_df['TotalBsmtSF'], y = train_df['SalePrice'])\n",
    "plt.ylabel('SalePrice')\n",
    "plt.xlabel('TotalBsmtSF')\n",
    "plt.show()\n",
    "\n",
    "#That data point looked like an outlier to TotalBsmtSF was the same point we just dropped - so we are good\n",
    "\n",
    "# feature OverallQual\n",
    "plt.subplots(figsize=(8, 6))\n",
    "sns.boxplot(x=train_df['OverallQual'], y=train_df[\"SalePrice\"])\n",
    "\n",
    "#Lets treat this as a category rather than a numerical value\n",
    "\n",
    "train_df['OverallCond'] = train_df['OverallCond'].astype(str)\n",
    "test_df['OverallCond'] = test_df['OverallCond'].astype(str)\n",
    "\n",
    "plt.scatter(x = train_df['TotalBsmtSF'], y = train_df['SalePrice'])\n",
    "plt.ylabel('SalePrice')\n",
    "plt.xlabel('TotalBsmtSF')\n",
    "plt.show()\n",
    "\n",
    "train_labels = train_df['SalePrice'].copy()\n",
    "\n",
    "train = train_df.drop('SalePrice', axis=1)\n",
    "test = test_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "cat_attribs = train.select_dtypes(include=[np.object]).columns\n",
    "train[cat_attribs].describe(include='all')\n",
    "\n",
    "train['Street'].value_counts()\n",
    "train['Utilities'].value_counts()\n",
    "\n",
    "#lets drop the above two categorical features, there is no variation no information here to learn from\n",
    "\n",
    "\n",
    "\n",
    "train.drop(columns=['Utilities', 'Street'], inplace=True)\n",
    "test.drop(columns=['Utilities', 'Street'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)\n",
    "\n",
    "# From the description of data MSSubClass looks like a categorical value\n",
    "train[['MSSubClass', 'YrSold', 'MoSold']] = train[['MSSubClass', 'YrSold', 'MoSold']].astype(str)\n",
    "test[['MSSubClass', 'YrSold', 'MoSold']] = test[['MSSubClass', 'YrSold', 'MoSold']].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: got ideas from https://www.kaggle.com/niteshx2/top-50-beginners-stacking-lgb-xgb\n",
    "\n",
    "def fix_nas(df):\n",
    "    #Fill na for these column with standard equipment for those - intuition from data description.txt\n",
    "    df['Functional'] = df['Functional'].fillna('Typ') \n",
    "    df['Electrical'] = df['Electrical'].fillna('SBrkr')\n",
    "    df['KitchenQual'] = df['KitchenQual'].fillna('TA')\n",
    "    df['SaleType'] = df['SaleType'].fillna('Other')\n",
    "    df['Exterior1st'] = df['Exterior1st'].fillna('Other')\n",
    "    df['Exterior2nd'] = df['Exterior2nd'].fillna('Other')\n",
    "    \n",
    "    #None for not exists - intuition from data description.txt\n",
    "    df['PoolQC'] = df['PoolQC'].fillna('None')\n",
    "    \n",
    "    #These two are probably very related\n",
    "    df['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    \n",
    "    #Na means No Garage\n",
    "    for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    #Na means No Garage \n",
    "    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "        df[col] = df[col].fillna('None')\n",
    "    \n",
    "    #Na means there's no basement\n",
    "    for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "        df[col] = df[col].fillna('None')\n",
    "    \n",
    "    #Na means no basement, so the measurement is 0\n",
    "    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "        df[col] = df[col].fillna(0)\n",
    "        \n",
    "    for col in train.select_dtypes(include=[np.object]).columns:\n",
    "        df[col] = df[col].fillna('None')\n",
    "        \n",
    "    for col in train.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = fix_nas(train_df)\n",
    "test_df = fix_nas(test_df)\n",
    "\n",
    "num_attribs = train.select_dtypes([np.number]).columns\n",
    "\n",
    "skewed_cols = num_attribs[train[num_attribs].skew() > 0.7]\n",
    "train[skewed_cols] = np.log1p(train[skewed_cols])\n",
    "test[skewed_cols] = np.log1p(test[skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(df):\n",
    "    \n",
    "    # Exists or not\n",
    "    df['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['Has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['HasBsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df['HasFireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    #Unfinished or not\n",
    "    df['BsmtFinType1_Unf'] = 1*(df['BsmtFinType1'] == 'Unf')\n",
    "    \n",
    "    df['OldHouse'] = df['YearBuilt'].apply(lambda x: 1 if x <1990 else 0)\n",
    "    \n",
    "    #Some aggregated features\n",
    "    df['TotalSF']=df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df['TotalBathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n",
    "                            df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n",
    "    df['TotalPorchSF'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n",
    "                          df['EnclosedPorch'] + df['ScreenPorch'] +\n",
    "                          df['WoodDeckSF'])\n",
    "    \n",
    "    \n",
    "  \n",
    "    df['Age_YrBuilt'] = df['YrSold'] - df['YearBuilt']\n",
    "    df['Age_YrRemod'] = df['YrSold'] - df['YearRemodAdd']\n",
    "    df['Age_Garage'] = df['YrSold'] - df['GarageYrBlt']\n",
    "    df['Remodeled'] = df['YearBuilt']!=df['YearRemodAdd']\n",
    "\n",
    "    #to fix if the garageyrbuilt is 0 for example - its 0 if garage was never built\n",
    "    df['Age_YrBuilt'] = df['Age_YrBuilt'].apply(lambda x: 0 if x <0 else x)\n",
    "    df['Age_YrRemod'] = df['Age_YrRemod'].apply(lambda x: 0 if x <0 else x)\n",
    "    df['Age_Garage'] = df['Age_Garage'].apply(lambda x: 0 if x <0 else x)\n",
    "        \n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = combine_features(train_df)\n",
    "test_df = combine_features(test_df)\n",
    "\n",
    "corrmat = train_df.corr()\n",
    "corrmat['SalePrice'].sort_values(ascending=False)[:25]\n",
    "\n",
    "corrmat['SalePrice'].sort_values(ascending=True)[:25]\n",
    "\n",
    "#Lets drop the two least correlated features\n",
    "train.drop(columns=['BsmtFinSF2', 'BsmtHalfBath'], inplace=True)\n",
    "test.drop(columns=['BsmtFinSF2', 'BsmtHalfBath'], inplace=True)\n",
    "\n",
    "#### Type of features\n",
    "num_attribs = train.select_dtypes([np.number]).columns\n",
    "\n",
    "ord_attribs = list(['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'GarageFinish', 'LandSlope', 'YrSold', 'MoSold'])\n",
    "\n",
    "cat_attribs = train.select_dtypes(include=[np.object]).columns\n",
    "\n",
    "print('numerical:{} \\n\\n categorical:{}'.format(num_attribs, cat_attribs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "        ('std_scaler', RobustScaler()),\n",
    "    ])\n",
    "\n",
    "# ord_pipeline = Pipeline([\n",
    "#         (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#         (\"encoder\", OrdinalEncoder()),\n",
    "#     ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown='ignore',sparse=False)),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "#         (\"ord\", ord_pipeline, ord_attribs),\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(train)\n",
    "\n",
    "train_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(max_iter=3000)\n",
    "lasso_reg = Lasso(max_iter=3000)\n",
    "\n",
    "param_grid = [{'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50]}]\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge_reg, param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "lasso_grid = GridSearchCV(lasso_reg, param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "ridge_grid.fit(train_prepared, train_labels)\n",
    "lasso_grid.fit(train_prepared, train_labels)\n",
    "\n",
    "lasso_scores = display_scores(lasso_grid.best_estimator_, train_prepared, train_labels)\n",
    "ridge_scores = display_scores(ridge_grid.best_estimator_, train_prepared, train_labels)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(min_samples_leaf=6, max_depth=4, max_features='sqrt', subsample=0.8, warm_start=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_prepared, train_labels)\n",
    "\n",
    "min_val_error = np.inf\n",
    "err_up = 0\n",
    "for n_estimators in range(1,1000):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        err_up = 0\n",
    "    else:\n",
    "        err_up += 1\n",
    "        if err_up == 5:\n",
    "            break\n",
    "\n",
    "gbrt_scores = display_scores(gbrt, train_prepared, train_labels)\n",
    "\n",
    "param_grid = [\n",
    "#         {'kernel': ['linear'], 'C': [0.1, 1, 10, 20]},\n",
    "        {'kernel': ['rbf'], 'C': [0.01, 1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0], 'gamma': [1e-6, 0.00001, 0.0003, 0.0001, 0.003, 0.01, 0.03, 0.1]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "svm_grid = GridSearchCV(svm_reg, param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "svm_grid.fit(train_prepared, train_labels)\n",
    "\n",
    "svm_scores = display_scores(svm_grid.best_estimator_, train_prepared, train_labels, cv=10)\n",
    "svm_grid.best_params_\n",
    "\n",
    "param_grid = {\n",
    "        'n_estimators': [50, 100, 250, 500, 1000],\n",
    "        'max_features': [32, 64, 128],\n",
    "        'max_depth': [2, 4, 8, 16, 32],\n",
    "        'bootstrap': [False, True],\n",
    "#         'max_samples': [0.25, 0.5, 0.75],\n",
    "        'min_samples_leaf': [2, 3, 5, 8],\n",
    "        'min_samples_split' : [2, 4, 8]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_reg, param_grid=param_grid, cv=4,\n",
    "                       scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "\n",
    "rf_grid.fit(train_prepared, train_labels)\n",
    "\n",
    "rf_grid.best_params_\n",
    "\n",
    "rf_scores = display_scores(rf_grid.best_estimator_, train_prepared, train_labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "param_distribs = {\n",
    "        'max_depth': randint(low=3, high=20),\n",
    "        'eta': [0.01, 0.05, 0.1],\n",
    "        'subsample' : [0.8, 1],\n",
    "        'colsample_bytree' : [0.3, 0.5, 0.8],\n",
    "        'n_estimators' : randint(low=400, high=1000),\n",
    "        'min_child_weight' : np.arange(1,6,2)\n",
    "    }\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(silent=True, early_stopping_rounds=5)\n",
    "\n",
    "xgb_grid = RandomizedSearchCV(xgb_reg, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', random_state=42)\n",
    "xgb_grid.fit(train_prepared, train_labels);\n",
    "\n",
    "xgb_scores = display_scores(xgb_grid.best_estimator_, train_prepared, train_labels, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = xgboost.XGBRegressor(early_stopping=7)\n",
    "\n",
    "estimators = [('r', ridge_grid.best_estimator_),\n",
    "              ('l', lasso_grid.best_estimator_),\n",
    "              ('boost', gbrt),\n",
    "              ('svm', svm_grid.best_estimator_),\n",
    "              ('rf', rf_grid.best_estimator_),\n",
    "              ('xgb', xgb_grid.best_estimator_)\n",
    "             ]\n",
    "\n",
    "stack = StackingRegressor(estimators=estimators, n_jobs=-1, passthrough=True)\n",
    "stack_2 = StackingRegressor(estimators=estimators, n_jobs=-1, passthrough=False)\n",
    "\n",
    "display_scores(stack, train_prepared, train_labels)\n",
    "stack_scores = display_scores(stack_2, train_prepared, train_labels)\n",
    "stack_2.fit(train_prepared, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended_predictions(X):\n",
    "    return ((0.1 * ridge_grid.best_estimator_.predict(X)) + \\\n",
    "            (0.2 * lasso_grid.best_estimator_.predict(X)) + \\\n",
    "            (0.1 * gbrt.predict(X)) + \\\n",
    "            (0.1 * xgb_grid.best_estimator_.predict(X)) + \\\n",
    "            (0.1 * svm_grid.best_estimator_.predict(X)) + \\\n",
    "            (0.05 * rf_grid.best_estimator_.predict(X)) + \\\n",
    "            (0.35 * stack_2.predict(X)))\n",
    "\n",
    "blended_mse = mean_squared_error(train_labels, blended_predictions(train_prepared))\n",
    "blended_score = np.sqrt(blended_mse)\n",
    "print('RMSLE score on train data:')\n",
    "print(blended_score)\n",
    "\n",
    "\n",
    "\n",
    "model = stack_2\n",
    "\n",
    "model.fit(train_prepared, train_labels)\n",
    "prepared = full_pipeline.transform(test)\n",
    "\n",
    "# preds = model.predict(prepared)\n",
    "preds = blended_predictions(prepared)\n",
    "preds_transformed = np.expm1(preds)\n",
    "preds_df = pd.DataFrame({'Id': test_ids, 'SalePrice': preds_transformed})\n",
    "preds_df.to_csv('submission_' + model.__class__.__name__ + '.csv', index=False)\n",
    "\n",
    "model.fit(train_prepared, train_labels)\n",
    "# train_preds = model.predict(train_prepared)\n",
    "train_preds = blended_predictions(train_prepared)\n",
    "mse = mean_squared_error(train_labels, train_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ridge_grid, lasso_grid, xgb_grid, svm_grid, gbrt, rf_grid, stack]\n",
    "for model in models:\n",
    "    save_model(model, fname=model.__class__.__name__ + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
