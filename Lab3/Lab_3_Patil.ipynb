{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import pdfminer\n",
    "import glob\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "#This block is just for importing relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Read Shannon's 1948 paper 'A Mathematical Theory of Communication'. Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication. http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf Summarize what you learned briely (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon's paper, \"A Mathematical Theory of Communication\", aims to tackle a fundamental problem of communication (as of 1948) relating to noiseless communication systems. The first part of the paper provides a brief intuition for the choice of the logarithmic function for information transfer, and then defines a communication system in terms of five components. The first two components are information source which provides the information to be transmit, and transmitter, which encodes the message in the form of a signal that can be transmit. The last two components are receiver and destination, which are the inverse of the transmitter and source, respectively. The transmitter and receiver are connected by a channel which is the medium used to transmit the signal, and that is where noise is likely to be introduced.\n",
    "\n",
    "The next few subsections look at the mathematial properties of discrete noiseless systems. Starting with the capacity of a channel, there is a brief discussion on allowable sequences, sources of information, approximations and n-grams. Generally, these sections try to lay the foundation for modern natural language processing (NLP), by talking about series of approximations to english, establishing sentences as a Markov (Markoff?) process and specifically, ergodic processes.\n",
    "\n",
    "After this formulation, a measure of uncertainity is introduced. This measure is entropy, and it has to follow three properties relating to the probaility distribution, i.e., continuity, monotonically increasing funcion of number of choices and indifference to successive choices. The only function satisfying said properties is the proposed \"Shannon\" formula which defines entropy as:\n",
    "\n",
    "$H = -K \\Sigma_{i=1}^n p_i logp_i$\n",
    "\n",
    "A few properties of this formula are explored such as behavior at extremes, behavior under joint distributions, conditional entropy, etc. The last few subsections talk about application of entropy to an information source, and how encoding/decoding operations can be represented to minimize the number of required bits. The fundamental theorem for a noiseless channel provides a hard upper bound for the avergae symbols per second transmitted for a given channel. Lastly, an example is provided to show how the average number of bits is obtained for a toy example with a special encoding scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "## Scraping, Entropy and ICML papers\n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2017 papers\n",
    "from http://proceedings.mlr.press/v70/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "\n",
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of Z.\n",
    "\n",
    "3. Synthesize a random paragraph using the marginal distribution over words.\n",
    "\n",
    "4. (Extra credit) Synthesize a random paragraph using an n-gram model on words. Synthesize a random paragraph using any model you want. Top five synthesized text paragraphs win bonus (+30 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "This code block below reads in all pdf files and pastes the plaintext into one txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\achab17a.pdf\n",
      "\n",
      "Working on: C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\acharya17a.pdf\n",
      "\n",
      "Working on: C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\achiam17a.pdf\n",
      "\n",
      "Working on: C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\agarwal17a.pdf\n",
      "\n",
      "Working on: C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\akrour17a.pdf\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b8164c76b467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mpdfConverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPdfConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m#print(pdfConverter.convert_pdf_to_txt())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mpdfConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_convert_pdf_to_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-b8164c76b467>\u001b[0m in \u001b[0;36msave_convert_pdf_to_txt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# convert pdf file text to string and save as a text_pdf.txt file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0msave_convert_pdf_to_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m        \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_pdf_to_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m        \u001b[0mtxt_pdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text_pdf.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m        \u001b[0mtxt_pdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b8164c76b467>\u001b[0m in \u001b[0;36mconvert_pdf_to_txt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m        \u001b[0mpagenos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxpages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_extractable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m            \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m        \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pdfminer\\converter.py\u001b[0m in \u001b[0;36mend_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLTPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageno\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_item\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pdfminer\\layout.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, laparams)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtextobjs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m         \u001b[0mtextlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlaparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mempties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextlines\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mempties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pdfminer\\layout.py\u001b[0m in \u001b[0;36mgroup_objects\u001b[1;34m(self, laparams, objs)\u001b[0m\n\u001b[0;32m    617\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mobj0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_voverlap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlaparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_overlap\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                     \u001b[1;33m<\u001b[0m \u001b[0mobj0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoverlap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mobj0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[1;33m<\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlaparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_margin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pdfminer\\layout.py\u001b[0m in \u001b[0;36mvoverlap\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvoverlap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLTComponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_voverlap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class PdfConverter:\n",
    "\n",
    "   def __init__(self, file_path):\n",
    "       self.file_path = file_path\n",
    "# convert pdf file to a string which has space among words \n",
    "   def convert_pdf_to_txt(self):\n",
    "       rsrcmgr = PDFResourceManager()\n",
    "       retstr = StringIO()\n",
    "       codec = 'utf-8'  # 'utf16','utf-8'\n",
    "       laparams = LAParams()\n",
    "       device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "       fp = open(self.file_path, 'rb')\n",
    "       interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "       password = \"\"\n",
    "       maxpages = 0\n",
    "       caching = True\n",
    "       pagenos = set()\n",
    "       for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "           interpreter.process_page(page)\n",
    "       fp.close()\n",
    "       device.close()\n",
    "       str = retstr.getvalue()\n",
    "       retstr.close()\n",
    "       return str\n",
    "# convert pdf file text to string and save as a text_pdf.txt file\n",
    "   def save_convert_pdf_to_txt(self):\n",
    "       content = self.convert_pdf_to_txt()\n",
    "       txt_pdf = open('text_pdf.txt', 'ab')\n",
    "       txt_pdf.write(content.encode('utf-8'))\n",
    "       txt_pdf.close()\n",
    "        \n",
    "pdflist = glob.glob(r\"C:\\Users\\priya\\Dropbox\\Sorted\\UT Austin Academics\\Fall 20\\EE 460 Data science lab\\Lab 3\\ICML Papers\\*.pdf\")\n",
    "\n",
    "for pdf in pdflist:\n",
    "    print(\"Working on: \" + pdf + '\\n')\n",
    "    pdfConverter = PdfConverter(file_path=pdf)\n",
    "    #print(pdfConverter.convert_pdf_to_txt())\n",
    "    pdfConverter.save_convert_pdf_to_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 150968),\n",
       " ('of', 76160),\n",
       " ('and', 66081),\n",
       " ('in', 54055),\n",
       " ('to', 50705),\n",
       " ('cid', 46285),\n",
       " ('is', 40360),\n",
       " ('for', 36792),\n",
       " ('we', 34037),\n",
       " ('that', 24842)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the stored plain text from pdfs\n",
    "with open (\"text_pdf.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "    text=myfile.readlines()\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# Tokenize and build vocabulary from the text corpus\n",
    "vectorizer.fit(text)\n",
    "# Summarize the vocabulary\n",
    "data = vectorizer.vocabulary_\n",
    "\n",
    "# This writes the words and counts to a txt file\n",
    "with open('Word_counts.txt', 'w', encoding = \"utf-8\") as f:\n",
    "    print(vectorizer.vocabulary_, file=f)\n",
    "\n",
    "# A function is defined to return top n frequent words from a vocabulary list\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "get_top_n_words(text, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2374466328\n",
      "The Shannon entropy is calculated to be:  15.793825595633031\n"
     ]
    }
   ],
   "source": [
    "# This converts the counts to raw probabilities of appearance, and drops zero value words, i.e., so rare that their probability was rounded down to zero.\n",
    "Prob_dist_raw = list(vectorizer.vocabulary_.values())\n",
    "Prob_dist = []\n",
    "sum_prob = sum(Prob_dist_raw)\n",
    "for x in Prob_dist_raw:\n",
    "    Prob_dist.append(x/sum_prob)\n",
    "#print(Prob_dist)\n",
    "Prob_dist = [i for i in Prob_dist if i != 0]\n",
    "\n",
    "# Entropy calculation below\n",
    "entropy = 0\n",
    "for i in range(len(Prob_dist)):\n",
    "    entropy = entropy + (Prob_dist[i]* math.log2(Prob_dist[i]))\n",
    "entropy = entropy*(-1)\n",
    "print(\"The Shannon entropy is calculated to be: \",entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subtract' 'biological' 'mineig' 'proprietary' 'freq' 'kushal'\n",
      " 'deeplearningbook' 'nowcasting' 'abilities' 'multimodal' 'χi' 'timally'\n",
      " 'zerocomponentsinπ' 'tiallyhigherthanitsaccuracy' 'mfccs' 'lindsten'\n",
      " 'fewcigarettesharmhealth' 'msr' 'uldas' 'ofﬁce' 'αpe' 'ues' 'u44' 'qi'\n",
      " 'wherex' 'wiki10' 'yehuda' 'vaguely' 'friends' 'themis' 'gigabytes'\n",
      " 'maharaj' 'teriori' 'unde' 'αpe' 'cussed' 'tilt' 'ρ2x2' 'swaps'\n",
      " 'emulator' 'spaces1' 'recsys' 'memorisation' 'owing' 'κj' 'kathuria'\n",
      " 'tuat' 'sαs' 'satb' 'zhenlong' 'τst' 'los' 'σd' 'ernment' 'ˆφk'\n",
      " 'auxiliary' 'paving' 'bishop' 'selc' '991flop' 'roneously' 'version2'\n",
      " 'wardsmoreintrinsicallymotivatedsolutionstocontinue' 's4'\n",
      " 'whilethechatteringpersists' 'bothofthesedatasetshaveverylowerror' 'sθi'\n",
      " 'purely' 'eliassi' 'polymorphism' 'c1n' 'popped' 'colmenarejo' 'harsh'\n",
      " 'xxxi' 'x0sq' 'unblocked' 'operations' 'sivity' 'longing' 'nottingham'\n",
      " 'inde' 'mehlhorn' 'dγit' 'huck' 'akinori' 'γ2' 'kstms' 'heave' 'gpus'\n",
      " 'les' 'dπ' 'parameterizationmemory' 'icar' 'packard' 'approximator2'\n",
      " 'ionization' 'rcnns' 'traversals' 'hypeprlanes']\n"
     ]
    }
   ],
   "source": [
    "# The block below returns n words chosen according to their probability of appearance\n",
    "# This is a \"paragraph\" of 100 words.\n",
    "n=100\n",
    "keys = np.array(list(vectorizer.vocabulary_.keys()))\n",
    "Probs = list(vectorizer.vocabulary_.values())\n",
    "sum_prob = sum(Probs)\n",
    "for x in range(len(Probs)):\n",
    "    Probs[x] = Probs[x]/sum_prob\n",
    "np.random.seed(2)\n",
    "choice_list = np.random.choice(keys, n, replace=True, p=Probs)\n",
    "print(choice_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The n-gram model is training now...\n",
      "The model has been trained successfully. The details are as follows:\n",
      "<NgramCounter with 5 ngram orders and 34267605 ngrams>\n"
     ]
    }
   ],
   "source": [
    "# This block below tokenizes the word corpus\n",
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n",
    "\n",
    "\n",
    "#This tokenizes our text saved in variable text\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(str(text))]\n",
    "\n",
    "# Preprocess the tokenized text for n-grams language modelling\n",
    "n = 5\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "\n",
    "model = MLE(n)\n",
    "print(\"The n-gram model is training now...\")\n",
    "model.fit(train_data, padded_sents)\n",
    "print(\"The model has been trained successfully. The details are as follows:\")\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random sentence number  0  is: \n",
      "probability of error and sim-\\n' ,' ple is used for neural variational inference.\n",
      "The random sentence number  1  is: \n",
      "' modeling that is entirely convolutional.\n",
      "The random sentence number  2  is: \n",
      "was used, as we had stability issues with smaller\\n' ,' network rnns trained on the tidigits digit recognition\\n' ,' \\n' ,' 80\\n' ,' \\n' ,' where zt = β1zt−1 + (1 − ρt) αu) ] \\n' ,' the graph was important for good predictive performance.\\n' ,' \\n' ,' x1, x2, . . ., k.\n",
      "The random sentence number  3  is: \n",
      "+\\n' ,' \\n' ,' \\n' ,' the main motivation for consid-\\n' ,' ered, the training cost can be reduced to the optimization problem, so running the model in real-time may\\n' ,' be learned from data.\\n' ,' as well as when α grows sub-linearly to n .\\n' ,' \\n' ,' ≤ (cid : 107) xs\\n' ,' (cid : 88) \\n' ,' i=1\\n' ,' \\n' ,' \\n' ,' and o( n2) space\\n' ,' embeddings of entities and relations can be\\n' ,' \\n' ,' isurvive, has three components: (1) while keeping the same func-\\n' ,' tion 7, we validate our theoretical results about\\n' ,' mean predicator\\n' ,' weisfeiler-lehman kernel with base kernel\\n' ,' \\x00( \\n' ,' theano-fft. https://github.com/andersbll/theano ops.\\n' ,' \\n' ,' \\n' ,' \\x00\\n' ,' since we would like to thank jeff johnson for his help\\n' ,' \\n\n",
      "The random sentence number  4  is: \n",
      "), but can potentially\\n' ,' re-weight the units in the layer directly below, but\\n' ,' scientiﬁcally interesting) associations between\\n' ,' discrepancy between barycenters and time series, at predic-\\n' ,' tion length (mdl) principle, \\n' ,' b( n) = a( n).\n",
      "The random sentence number  5  is: \n",
      "and\\n' ,' sridharan , 2013; seldin and slivkins , 2014) or medical treatment design\\n' ,' \\n' ,' 1% of generated sequences proved to be a valid molecule\\n' ,' encoding.\\n' ,' \\n' ,' i\\n' ,' \\n' ,' blkdiag\\n' ,' \\n' ,' sennrich, rico, haddow, barry, huck, matthias, jimeno-yepes, \\n' ,' u\\n' ,' \\n' ,' z = blstm( {¯xt} t\\n' ,' (α0 − α) 2\\n' ,' 1 ←...←\\n' ,' φi\\n' ,' \\n' ,' and ﬂexibility of the cdp framework.\n",
      "The random sentence number  6  is: \n",
      "models to have been described in\\n' ,' the expectation of φ over samples with dependencies by the\\n' ,' statements\\n' ,' j: , (cid : 98) h u, v\\n' ,' i\\n' ,' \\n' ,' (kruskal , 1958): suppose (x, y) (cid : 107) t v\\n' ,' ≤\\n' ,' \\n' ,' where vh denotes the head of the distribution to be assigned to\\n' ,' approximate algorithms: \\n' ,' zoran, d. and weiss, y.\n",
      "The random sentence number  7  is: \n",
      ",' kapoor, ashish, and deshpande, \\n' ,' the chain mdp example mentioned in\\n' ,' the farthest room in this map (sparse reward).\\n' ,' \\n' ,' and foot, along with 3 actuated joints.\n",
      "The random sentence number  8  is: \n",
      ") ∆i( sr) = max\\n' ,' at∈it\\n' ,' \\n' ,' \\n' ,' //arxiv.org/abs/1504.00941.\\n' ,' \\n' ,' mirza, mehdi and osindero, simon.\n",
      "The random sentence number  9  is: \n",
      "\n",
      "The random sentence number  10  is: \n",
      "\\n' ,' t\\n' ,' \\n' ,' ✏) approx-\\n' ,' imation if i) the support of the poisson is in-\\n' ,' ferring to reason at a higher level of abstraction (botvinick\\n' ,' et al. , 2008), mainly\\n' ,' because they they do not impact the dual number\\n' ,' of the random variables\\n' ,' rxz\\n' ,' quasi-instrumental variables or quasi-ivs for short.\\n' ,' \\n' ,' \\x00\\n' ,' \\n' ,' \\x00\\n' ,' 2yi + 3yj\\n' ,' 4.\n",
      "The random sentence number  11  is: \n",
      "\n",
      "The random sentence number  12  is: \n",
      "\n",
      "The random sentence number  13  is: \n",
      ",' max\\n' ,' \\n' ,' elsewhere\\n' ,' \\n' ,' 18m 3m\\n' ,' \\n' ,' tion.\\n' ,' \\n' ,' 0.09\\n' ,' 0.46\\n' ,' 3.65 ± 0.40\\n' ,' 0.40\\n' ,' 0.40\\n' ,' 0.40\\n' ,' 0.40\\n' ,' 0.10\\n' ,' 0.10\\n' ,' \\n' ,' o (poly (log (1/( cid : 15) -top-k arms.\n",
      "The random sentence number  14  is: \n",
      "' \\x00\\n' ,' \\n' ,' max\\n' ,' \\n' ,' sworker, \\n' ,' kemp, c., tenenbaum, j.\n",
      "The random sentence number  15  is: \n",
      "which\\n' ,' would additionally like the baseline to convey a complete\\n' ,' the svm-sgd learning algorithm.\n",
      "The random sentence number  16  is: \n",
      ",' a list of documents (chuklin et al. , 2015; dinh\\n' ,' et al. , 1997) and gated recur-\\n' ,' rent neural networks improves its ability to perform long-\\n' ,' studied in (yong , 2015).\n",
      "The random sentence number  17  is: \n",
      "for this dual optimization over y we uti-\\n' ,' lized for approximating huge matrices (cheng et al. , 2015) to conduct experiments on en↔fr and\\n' ,' ﬁlters.\n",
      "The random sentence number  18  is: \n",
      "' \\n' ,' cho, kyunghyun, and bengio, yoshua.\n",
      "The random sentence number  19  is: \n",
      "data set d( cid : 63) ).\n"
     ]
    }
   ],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sentence(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"The random sentence number \",i,\" is: \")\n",
    "    print(generate_sentence(model, 200, random_seed=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "## Starting in Kaggle\n",
    "\n",
    "Soon you will be participating in the in-class Kaggle competition made for this class. In that one, you will be participating on your own. This is a warmup- the more e\u000b",
    "ort and research you put into this assignment the easier it will be to compete into the real Kaggle competition that you will need to do soon. We expect you to spend 10 times more e\u000b",
    "ort on this problem compared to the others.\n",
    "\n",
    "1. Let's start with our first Kaggle submission in a playground regression competition. Make an account to Kaggle and find https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "\n",
    "2. Follow the data preprocessing steps from https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models. Then run a ridge regression using $\\alpha = 0.1$. Make a submission of this prediction, what is the RMSE you get? (Hint: remember to exponentiate np.expm1(ypred) your predictions).\n",
    "\n",
    "3. Compare a ridge regression and a lasso regression model. Optimize the alphas using cross validation. What is the best score you can get from a single ridge regression model and from a single lasso model?\n",
    "\n",
    "4. Plot the l0 norm (number of nonzeros) of the coefficients that lasso produces as you vary the strength of regularization parameter alpha.\n",
    "\n",
    "5. Add the outputs of your models as features and train a ridge regression on all the features plus the model outputs (This is called Ensembling and Stacking). Be careful not to overfit. What score can you get? (We will be discussing ensembling more, later in the class, but you can start playing with it now).\n",
    "\n",
    "6. Install XGBoost (Gradient Boosting) and train a gradient boosting regression. What score can you get just from a single XGB? (you will need to optimize over its parameters). We will discuss boosting and gradient boosting in more detail later. XGB is a great friend to all good Kagglers!\n",
    "\n",
    "7. Do your best to get the more accurate model. Try feature engineering and stacking many models. You are allowed to use any public tool in python. No non-python tools allowed.\n",
    "\n",
    "8. (Optional) Read the Kaggle forums, tutorials and Kernels in this competition. This is an excellent way to learn. Include in your report if you find something in the forums you like, or if you made your own post or code post, especially if other Kagglers liked or used it afterwards.\n",
    "\n",
    "9. Be sure to read and learn the rules of Kaggle! No sharing of code or data outside the Kaggle forums. Every student should have their own individual Kaggle account and teams can be formed in the Kaggle submissions with partners. This is more important for live competitions of course.\n",
    "\n",
    "10. As in the real in-class Kaggle competition (which will be next), you will be graded based on your public score (include that in your report) and also on the creativity of your solution. In your report (that you will submit as a pdf file), explain what worked and what did not work. Many creative things will not work, but you will get partial credit for developing them. We will invite teams with interesting solutions to present them in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "#This block reads the necessary input files\n",
    "\n",
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "\n",
    "\n",
    "print(train.head())\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], test.loc[:,'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001D50D54DA20>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001D50D512F60>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data pre-processing as given in link\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "prices.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13777538277187837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge = Ridge()\n",
    "\n",
    "alpha = 0.1\n",
    "cv_ridge = rmse_cv(Ridge(alpha = alpha)).mean()\n",
    "\n",
    "print(cv_ridge)\n",
    "\n",
    "model_ridge.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This block is for making prediction using ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model_ridge.predict(X_test)\n",
    "\n",
    "SalePrice = np.expm1(ypred)\n",
    "\n",
    "SalePrice_arr = [\"Id\"]\n",
    "for i in range(len(SalePrice)):\n",
    "    SalePrice_arr.append(SalePrice[i])\n",
    "    \n",
    "Id_list = [\"SalePrice\"]\n",
    "for i in range(len(test.Id)):\n",
    "    Id_list.append(test.Id[i])\n",
    "    \n",
    "f = open(\"Ridge.csv\", \"w\")\n",
    "f.write(str(Id_list))\n",
    "f.write(\"\\n\")\n",
    "f.write(str(SalePrice_arr))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Score\n",
    "\n",
    "This provides a Kaggle score of 0.13043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
    "rmse_cv(model_lasso).mean()\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
